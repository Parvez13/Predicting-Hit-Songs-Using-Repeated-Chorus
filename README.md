# Predicting-Hit-Songs-Using-Repeated-Chorus
We will examine this myth by creating a data set of choruses from popular artists and applied five supervised Machine Learning techniques to predict the popularity purely based on the audio features extracted from the first 15 second chorus.

## Understanding the Dataset
The dataset we are working on was collected using multiple Libraries    
The Songs dataset contains a total of **520** audio features extracted from the pychorus and librosa package, and a total of **750** popular and unpopular songs data from **2006** to **2021**.

## Preprocessing of the data
For Collection and preprocessing of the data includes the following steps:
  1. Firstly we got the list of yearly most popular artists from ***2006*** to ***2021***.
  2. Then we used billboard's python package to get the list of popular and unpopular songs of the selected artists in each year from ***2006*** to ***2021***.
  3. Then we used youtube-dl python package to download all the songs
  4. Once all of the songs were downloaded we started the extraction process using pychorus package and created a dataframe of the collected data.
  
Once the Data was ready before modelling we used 2 of the categorical columns to be fed into ordinal encoder function and column Transformer of scikit-learns package and transformed the Title of the songs as well as the Artists of the songs.
```
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer

ordinal = OrdinalEncoder()

transform = ColumnTransformer([(
             "ordinal",ordinal,categorical_features)],
             remainder='passthrough')
transformed_X = transform.fit_transform(X)
transformed_X

```
**fit_transform()** is used on the training data so that we can encode the training data and also learn the encoding parameters of that data. These learned parameters are then used to encode our test data.

**transform()** uses the same encoding parameters as it is calculated from our training data to transform our test data. Thus, the parameters learned by our Encoder using the training data will help us to transform our test data. As we do not want to be biased with our model, but we want our test data to be a completely new and a surprise set for our model.


## EDA
**Introduction:**

- **Complete dataset** comprises of 751 rows and 523 columns.
- Dataset comprises of continious variable and float data type. 
- Categorical Columns included Artist Name and Song Title.
- Remaining of the columns were created by librosa library.

**Visual Analysis:**

Plotted Pair plot to see the distribution of median of each column and found that most of the variables are normally randomly. However, we can't really say about that which variables needed to be studied. Since the number of columns were alot and evaluation of each variable is not neccessary.

**Principal Component Analysis:**
- PCA was done to extract the columns which can explain 95% of the variance in the extracted dataset. 
- A PCA model was built and then the data without the labels were fit to be transformed and a biplot was visualized with a total of 400 components. 
- PCA returned transformed data which indicated 171 columns were explaining 95% of the variance in our dataset. 
- Since, Every feature was important we didnot dropped out low performing features in our PCA and built our Prediction model on a total of 520 Columns. 

**Pandas Profiling:**

- Pandas profiling gave us a detailed insight on our data
- Profiling was done on the insightful data generated by our PCA model
 
## Model Building

#### Metrics considered for Model Evaluation
**Accuracy , Precision , Recall and F1 Score**
- Accuracy: What proportion of actual positives and negatives is correctly classified?
- Precision: What proportion of predicted positives are truly positive ?
- Recall: What proportion of actual positives is correctly classified ?
- F1 Score : Harmonic mean of Precision and Recall

#### Logistic Regression
- Logistic Regression helps find how probabilities are changed with actions.
- The function is defined as P(y) = 1 / 1+e^-(A+Bx) 
- Logistic regression involves finding the **best fit S-curve** where A is the intercept and B is the regression coefficient. The output of logistic regression is a probability score.

#### Random Forest Classifier
- The random forest is a classification algorithm consisting of **many decision trees.** It uses bagging and features randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.
- **Bagging and Boosting**: In this method of merging the same type of predictions. Boosting is a method of merging different types of predictions. Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance.
- **Feature Randomness**:  In a normal decision tree, when it is time to split a node, we consider every possible feature and pick the one that produces the most separation between the observations in the left node vs. those in the right node. In contrast, each tree in a random forest can pick only from a random subset of features. This forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification.


#### Random Search CV
- Random Search CV is a cross validation techniques which uses a grid of specified parameters of the selected model
- when random search cv is fitted on the data it randomly selects the hyperparameters of the model from the specified grid and fit the training data on it.
- these random sampling of hyper parameter tuning is done until a good score is reached or all possible outcomes of the hyper parameters have been tested.
- once it is completed it shows the best performing hyper parameters and the best score achieved by tuning the model.


## Deployment
you can access our app by following this link [Predicting-Hit-Songs-Heroku](https://song-prediction-using-chorus.herokuapp.com/)

### Flask 
- Flask is a python web development framework which we used to create and deploy our model
- Flask creates a web application which runs on the local server or local host of the system on which it is being created.
- to deploy flask application on internet we have to use deployment providers, for which we used Heroku as a deployment partner.
- The files of flask web app are located into (Flask_deployment) folder.
- the same folder also contains pickle files of our model and encoders which we trained on our training data.

### Heroku
We deploy our Flass web app to [ Heroku.com](https://www.heroku.com/). In this way, we can share our app on the internet with others. 
We prepared the needed files to deploy our app sucessfully:
- Procfile: contains run statements for app file and setup.sh.
- requirements.txt: contains the libraries must be downloaded by Heroku to run app file (app.py) successfully 
- app.py: contains the python code of a Flask web app.
- extract_chorus.py: contains the function to extract the chorus of the selected song.
- extract_audio_feature.py: contains the function to extract audio features of the song selected by the user.
- transform.pkl: contains the column transformer data which was used to encode training data.
- model.pkl: contains our model that was built during our modeling part.
- Folder(mysong): contains the uploaded songs of the user which then will be used to futher preprocessing steps.  
